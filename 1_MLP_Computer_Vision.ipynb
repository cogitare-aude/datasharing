{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cogitare-aude/datasharing/blob/master/1_MLP_Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "oPR4ivrheD8_"
      },
      "source": [
        "# Logistic (softmax) regression model\n",
        "\n",
        "This notebook is organised in two parts:\n",
        "\n",
        "* the *first part* implements a **logistic regression model from scratch**, as we did for the linear regression model.\n",
        "\n",
        "* the *second part* makes use of **high-level APIs** for a concise implementation of the same logistic regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note for ST456\n",
        "\n",
        "The following commands are necessary for downloading some helper functions in TensorFlow used by the reference book.\n",
        "\n",
        "If you get a message saying **you need to restart the runtime**, please **do so before** running the rest of the code."
      ],
      "metadata": {
        "id": "TJPYO8trsP6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/d2l\n",
        "! wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/tensorflow.py\n",
        "!mv tensorflow.py /content/d2l\n",
        "\n",
        "#!pip install d2l==0.17.1 2>/dev/null"
      ],
      "metadata": {
        "id": "vxZJpnn1YKnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e941ee41-b235-4ba2-c163-7d25b0363f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-27 21:57:52--  https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/tensorflow.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74148 (72K) [text/plain]\n",
            "Saving to: ‘tensorflow.py’\n",
            "\n",
            "\rtensorflow.py         0%[                    ]       0  --.-KB/s               \rtensorflow.py       100%[===================>]  72.41K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-01-27 21:57:53 (13.5 MB/s) - ‘tensorflow.py’ saved [74148/74148]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "tensorflow"
        ],
        "id": "VmKEfc0GeD9B"
      },
      "outputs": [],
      "source": [
        "# importing necessary modules\n",
        "import tensorflow as tf\n",
        "from IPython import display\n",
        "from d2l import tensorflow as d2l # helper module from the reference book"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. \n",
        "\n",
        "You can access/import the Fashion MNIST directly from TensorFlow. Here, we are setting up a data iterator with batch size 256."
      ],
      "metadata": {
        "id": "gt8LgjadxQLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 4,
        "tab": [
          "tensorflow"
        ],
        "id": "IJr8ZZAFeD9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0d7c8f-edc1-481f-ff0f-8bfc35c89693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# loading data in 256 samples\n",
        "batch_size = 256\n",
        "\n",
        "#create training (235) and testing (40) set with batch size = 256 (download data directly from TF)\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach: logistic regression from scratch\n",
        "\n"
      ],
      "metadata": {
        "id": "tK9PPI1yzYYf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "ObtX16W3eD9D"
      },
      "source": [
        "## Initializing model parameters\n",
        "\n",
        "As in our linear regression example,\n",
        "each example here will be represented by a fixed-length vector.\n",
        "Each example in the raw dataset is a $28 \\times 28$ image.\n",
        "**We will flatten each image,\n",
        "treating them as vectors of length 784**, so each pixel will be treated as just another feature.\n",
        "\n",
        "Recall that in softmax regression,\n",
        "we have as many outputs as there are classes.\n",
        "**Because our dataset has 10 classes,\n",
        "our network will have an output dimension of 10**.\n",
        "Consequently, our weights will constitute a $784 \\times 10$ matrix\n",
        "and the biases will constitute a $1 \\times 10$ row vector.\n",
        "As with linear regression, we will initialize our weights `W`\n",
        "with Gaussian noise and our biases to take the initial value 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "tensorflow"
        ],
        "id": "kYir3G_QeD9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1faaf4ff-132b-44f0-d1f5-0e7d3926b72b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(784, 10) dtype=float32, numpy=\n",
              "array([[-1.15743168e-02, -1.27553754e-03, -5.41199977e-03, ...,\n",
              "        -2.72890669e-03, -1.75411236e-02,  1.72494370e-02],\n",
              "       [-7.04754889e-03,  1.22556752e-02,  1.12869805e-02, ...,\n",
              "        -3.84268984e-02, -7.55167683e-04, -2.09961785e-03],\n",
              "       [ 1.10874102e-02,  4.32833144e-03, -3.49263755e-05, ...,\n",
              "         1.12616895e-02,  1.81942503e-03, -1.94550259e-03],\n",
              "       ...,\n",
              "       [-8.26264545e-03, -8.72868765e-03,  4.50238585e-03, ...,\n",
              "         1.25187617e-02,  5.27327647e-03, -6.79829158e-04],\n",
              "       [ 6.65959809e-03,  1.82154514e-02,  1.78031754e-02, ...,\n",
              "         1.75812673e-02,  7.37626012e-03,  3.42641049e-03],\n",
              "       [ 5.62647404e-03, -1.18888747e-02, -6.92862179e-03, ...,\n",
              "        -8.63544736e-03,  1.49093568e-04, -1.62499920e-02]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# input and output dimensions\n",
        "#input, ie. features (x1...x784), ea. image flattens to become pixels, each pixel = 1-feature, 1 image has 784 pixels\n",
        "#output, ie. y (where are 10 classes, shirt....pants)\n",
        "#also 784 row x 10 columns of weights (one weight for ea. class)\n",
        "#also bias 1 row x 10 column\n",
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "\n",
        "# initialising model parameters\n",
        "W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs), mean=0, stddev=0.01))\n",
        "b = tf.Variable(tf.zeros(num_outputs))\n",
        "\n",
        "W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "id": "m-5JIhAIeD9E"
      },
      "source": [
        "### Defining the softmax operation\n",
        "\n",
        "Before implementing the softmax regression model,\n",
        "let us briefly review how the sum operator works\n",
        "along specific dimensions in a tensor:\n",
        "\n",
        "**Given a matrix `X` we can sum over all elements (by default) or only\n",
        "over elements in the same axis**,\n",
        "i.e., the same column (axis 0) or the same row (axis 1).\n",
        "\n",
        "Note that if `X` is a tensor with shape (2, 3)\n",
        "and we sum over the columns,\n",
        "the result will be a vector with shape (3,).\n",
        "When invoking the sum operator,\n",
        "we can specify to keep the number of axes in the original tensor,\n",
        "rather than collapsing out the dimension that we summed over.\n",
        "This will result in a two-dimensional tensor with shape (1, 3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "tensorflow"
        ],
        "id": "p7SD561ReD9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0759d9b-79e3-4696-ec54-8bb5dd5af028"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[5., 7., 9.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
              " array([[ 6.],\n",
              "        [15.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# example of sum operator over a tensor\n",
        "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "tf.reduce_sum(X, 0, keepdims=True), tf.reduce_sum(X, 1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "WzXU5HJdeD9G"
      },
      "source": [
        "We are now ready to **implement the softmax operation**.\n",
        "Recall that softmax consists of three steps:\n",
        "\n",
        "1. we exponentiate each term (using `exp`);\n",
        "\n",
        "2. we sum over each row (we have one row per example in the batch)\n",
        "to get the normalization constant for each example;\n",
        "\n",
        "3. we divide each row by its normalization constant,\n",
        "ensuring that the result sums to 1.\n",
        "\n",
        "Before looking at the code, let us recall\n",
        "how this looks expressed as an equation:\n",
        "\n",
        "\n",
        "$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}$$\n",
        "\n",
        "The denominator, or normalization constant,\n",
        "is also sometimes called the *partition function*\n",
        "(and its logarithm is called the log-partition function).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 13,
        "tab": [
          "tensorflow"
        ],
        "id": "kixTd3SieD9H"
      },
      "outputs": [],
      "source": [
        "# custom implementation of the softmax function\n",
        "def softmax(X):\n",
        "    X_exp = tf.exp(X)\n",
        "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
        "    return X_exp / partition  # the broadcasting mechanism is applied here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 15,
        "id": "E-mGYYpyeD9H"
      },
      "source": [
        "As you can see, for any random input, **we turn each element into a non-negative number. Moreover, each row sums up to 1**, as is required for a probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "tensorflow"
        ],
        "id": "VN5XPYlpeD9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f73728-5b3c-40c1-de90-b90f168bd003"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
              " array([[0.12505676, 0.43577784, 0.05986837, 0.10240012, 0.2768969 ],\n",
              "        [0.07492542, 0.3339921 , 0.17761248, 0.19963543, 0.2138346 ]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.99999994, 1.0000001 ], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X = tf.random.normal((2, 5), 0, 1)\n",
        "X_prob = softmax(X)\n",
        "X_prob, tf.reduce_sum(X_prob, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 18,
        "id": "wfBfv7dqeD9I"
      },
      "source": [
        "Note that while this looks correct mathematically,\n",
        "we were a bit sloppy in our implementation\n",
        "because we failed to take precautions against numerical overflow or underflow\n",
        "due to large or very small elements of the matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### Defining the model\n",
        "\n",
        "Now that we have defined the softmax operation,\n",
        "we can **implement the softmax regression model**.\n",
        "The below code defines how the input is mapped to the output through the network.\n",
        "Note that we flatten each original image in the batch\n",
        "into a vector using the `reshape` function\n",
        "before passing the data through our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "tensorflow"
        ],
        "id": "AoCpDrqyeD9I"
      },
      "outputs": [],
      "source": [
        "# custom logistic regression model\n",
        "def net(X):\n",
        "    return softmax(tf.matmul(tf.reshape(X, (-1, W.shape[0])), W) + b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "gd4mHzWGeD9I"
      },
      "source": [
        "### Defining the loss function\n",
        "\n",
        "Next, we need to implement the **cross-entropy loss function**. This may be the most common loss function\n",
        "in all of deep learning because, at the moment,\n",
        "classification problems far outnumber regression problems.\n",
        "\n",
        "Recall that cross-entropy takes the negative log-likelihood\n",
        "of the predicted probability assigned to the true label.\n",
        "Rather than iterating over the predictions with a Python for-loop\n",
        "(which tends to be inefficient),\n",
        "we can pick all elements by a single operator.\n",
        "\n",
        "Below, we **create sample data `y_hat`\n",
        "with 2 examples of predicted probabilities over 3 classes and their corresponding labels `y`**. With `y` we know that in the first example the first class is the correct prediction and\n",
        "in the second example the third class is the ground-truth.\n",
        "**Using `y` as the indices of the probabilities in `y_hat`**, we pick the probability of the first class in the first example\n",
        "and the probability of the third class in the second example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "tensorflow"
        ],
        "id": "uhKMD3ZNeD9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eec1ced-550a-478a-8bb9-bc505faa16ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.1, 0.5], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# example data for demonstrating the cross-entropy loss and the accuracy\n",
        "y_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y = tf.constant([0, 2])\n",
        "tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 23,
        "id": "PVIn-R7IeD9J"
      },
      "source": [
        "Now we can **implement the cross-entropy loss function** efficiently with just one line of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "tensorflow"
        ],
        "id": "u-aaF0HgeD9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53fe7bc7-86a3-469e-cf75-e9ee3e2b9f67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.3025851, 0.6931472], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# custom implementation of the cross-entropy function\n",
        "def cross_entropy(y_hat, y):\n",
        "    return -tf.math.log(\n",
        "        tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1])))\n",
        "\n",
        "cross_entropy(y_hat, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 26,
        "id": "57fHXpyTeD9J"
      },
      "source": [
        "### Classification accuracy\n",
        "\n",
        "Given the predicted probability distribution `y_hat`,\n",
        "we typically choose the class with the highest predicted probability\n",
        "whenever we must output a hard prediction.\n",
        "Indeed, many applications require that we make a choice.\n",
        "Gmail must categorize an email into \"Primary\", \"Social\", \"Updates\", or \"Forums\".\n",
        "It might estimate probabilities internally,\n",
        "but at the end of the day it has to choose one among the classes.\n",
        "\n",
        "When predictions are consistent with the label class `y`, they are correct.\n",
        "**The classification accuracy is the fraction of all predictions that are correct**.\n",
        "Although it can be difficult to optimize accuracy directly (it is not differentiable),\n",
        "it is often the performance measure that we care most about,\n",
        "and we will nearly always report it when training classifiers.\n",
        "\n",
        "To compute accuracy we do the following:\n",
        "\n",
        "1. if `y_hat` is a matrix, we assume that the second dimension stores prediction scores for each class.\n",
        "\n",
        "2. we use `argmax` to obtain the predicted class by the index for the largest entry in each row.\n",
        "\n",
        "3. then we **compare the predicted class with the ground-truth `y` elementwise**. Since the equality operator `==` is sensitive to data types, we convert `y_hat`'s data type to match that of `y`.\n",
        "\n",
        "4. the result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 27,
        "tab": [
          "tensorflow"
        ],
        "id": "G_NFa687eD9K"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y):  \n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "    cmp = tf.cast(y_hat, y.dtype) == y\n",
        "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 28,
        "id": "kMgvHqnheD9K"
      },
      "source": [
        "We will continue to use the variables `y_hat` and `y`\n",
        "defined before\n",
        "as the predicted probability distributions and labels, respectively.\n",
        "We can see that the first example's prediction class is 2\n",
        "(the largest element of the row is 0.6 with the index 2),\n",
        "which is inconsistent with the actual label, 0.\n",
        "The second example's prediction class is 2\n",
        "(the largest element of the row is 0.5 with the index of 2),\n",
        "which is consistent with the actual label, 2.\n",
        "Therefore, the classification accuracy rate for these two examples is 0.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 29,
        "tab": [
          "tensorflow"
        ],
        "id": "O2taCRfTeD9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a175029-cc7a-448b-e2b5-1b7535a75565"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# example case\n",
        "accuracy(y_hat, y) / len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 30,
        "id": "weyNW6cseD9L"
      },
      "source": [
        "Similarly, we can **evaluate the accuracy for any model `net` on a dataset**] that is accessed via the data iterator `data_iter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 31,
        "tab": [
          "tensorflow"
        ],
        "id": "Cy6ZnpLfeD9L"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(net, data_iter): \n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), tf.size(y).numpy())\n",
        "    return metric[0] / metric[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 33,
        "id": "C9QMX98NeD9L"
      },
      "source": [
        "Here `Accumulator` is a utility class to accumulate sums over multiple variables.\n",
        "In the above `evaluate_accuracy` function,\n",
        "we create 2 variables in the `Accumulator` instance for storing both\n",
        "the number of correct predictions and the number of predictions, respectively.\n",
        "Both will be accumulated over time as we iterate over the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "tensorflow"
        ],
        "id": "krEH1haeeD9L"
      },
      "outputs": [],
      "source": [
        "# helper class for storing the number of correct predictions and the total number of predictions\n",
        "class Accumulator:  \n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 35,
        "id": "70JGP6SQeD9M"
      },
      "source": [
        "**Because we initialized the `net` model with random weights,\n",
        "the accuracy of this model should be close to random guessing**, i.e., 0.1 for 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 36,
        "tab": [
          "tensorflow"
        ],
        "id": "-YKB7w4keD9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8787601-bdcc-467a-9507-a9f542637fbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0839"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "evaluate_accuracy(net, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 37,
        "id": "Ods_BjGheD9M"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "The training loop for softmax regression should look similar to our implementation of linear regression.\n",
        "Here we refactor the implementation to make it reusable.\n",
        "First, we define a function to train for one epoch.\n",
        "Note that `updater` is a general function to update the model parameters,\n",
        "which accepts the batch size as an argument.\n",
        "It can be either a wrapper of the `d2l.sgd` function\n",
        "or a framework's built-in optimization function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 40,
        "tab": [
          "tensorflow"
        ],
        "id": "S0XQ5dXjeD9M"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net, train_iter, loss, updater):  \n",
        "    \"\"\"The training loop\"\"\"\n",
        "    # Sum of training loss, sum of training accuracy, no. of examples\n",
        "    metric = Accumulator(3)\n",
        "    for X, y in train_iter:\n",
        "        # Compute gradients and update parameters\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_hat = net(X)\n",
        "            # Keras implementations for loss takes (labels, predictions)\n",
        "            # instead of (predictions, labels) that users might implement,\n",
        "            # e.g. `cross_entropy` that we implemented above\n",
        "            if isinstance(loss, tf.keras.losses.Loss):\n",
        "                l = loss(y, y_hat)\n",
        "            else:\n",
        "                l = loss(y_hat, y)\n",
        "        if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
        "            params = net.trainable_variables\n",
        "            grads = tape.gradient(l, params)\n",
        "            updater.apply_gradients(zip(grads, params))\n",
        "        else:\n",
        "            updater(X.shape[0], tape.gradient(l, updater.params))\n",
        "        # Keras loss by default returns the average loss in a batch\n",
        "        l_sum = l * float(tf.size(y)) if isinstance(\n",
        "            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
        "        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
        "    # Return training loss and training accuracy\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 41,
        "id": "6RdjgEvpeD9N"
      },
      "source": [
        "Before showing the implementation of the training function,\n",
        "we define *a utility class that plot data in animation*.\n",
        "This is just for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 42,
        "tab": [
          "tensorflow"
        ],
        "id": "jiTNZRR_eD9N"
      },
      "outputs": [],
      "source": [
        "# helper function for animated data visualisation\n",
        "class Animator:  \n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        d2l.use_svg_display()\n",
        "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes,]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: d2l.set_axes(self.axes[\n",
        "            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 43,
        "id": "iJKTLR-FeD9N"
      },
      "source": [
        "The following training function then\n",
        "trains a model `net` on a training dataset accessed via `train_iter`\n",
        "for multiple epochs, which is specified by `num_epochs`.\n",
        "At the end of each epoch,\n",
        "the model is evaluated on a testing dataset accessed via `test_iter`.\n",
        "We will leverage the `Animator` class to visualize\n",
        "the training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 44,
        "tab": [
          "tensorflow"
        ],
        "id": "1pvuf8goeD9O"
      },
      "outputs": [],
      "source": [
        "def train_model(net, train_iter, test_iter, loss, num_epochs, updater):  \n",
        "    \"\"\"Train a model\"\"\"\n",
        "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "    for epoch in range(num_epochs):\n",
        "        # training step\n",
        "        train_metrics = train_epoch(net, train_iter, loss, updater)\n",
        "        # evaluation step\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\n",
        "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
        "    train_loss, train_acc = train_metrics\n",
        "    assert train_loss < 0.5, train_loss\n",
        "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
        "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 45,
        "id": "ncrRogRKeD9O"
      },
      "source": [
        "As an implementation from scratch,\n",
        "we use the **minibatch stochastic gradient descent** to optimize the loss function of the model with a learning rate 0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 47,
        "tab": [
          "tensorflow"
        ],
        "id": "P_KKu3h0eD9O"
      },
      "outputs": [],
      "source": [
        "# custom class for performing model optimisation\n",
        "class Updater():  \n",
        "    \"\"\"For updating parameters using minibatch stochastic gradient descent.\"\"\"\n",
        "    def __init__(self, params, lr):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def __call__(self, batch_size, grads):\n",
        "        d2l.sgd(self.params, grads, self.lr, batch_size)\n",
        "\n",
        "updater = Updater([W, b], lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 48,
        "id": "ueiUaEHjeD9O"
      },
      "source": [
        "Now we **train the model with 10 epochs**.\n",
        "Note that both the number of epochs (`num_epochs`),\n",
        "and learning rate (`lr`) are adjustable hyperparameters.\n",
        "By changing their values, we may be able\n",
        "to increase the classification accuracy of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 49,
        "tab": [
          "tensorflow"
        ],
        "id": "5lTzyW3BeD9P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "9e9aaa9b-a8aa-4ce7-fe96-d74aac8f38b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 238.965625 180.65625\" width=\"238.965625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 238.965625 180.65625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 143.1 \nL 225.403125 143.1 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 51.803125 143.1 \nL 51.803125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfde8259c35\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.803125\" xlink:href=\"#mfde8259c35\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(48.621875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 95.203125 143.1 \nL 95.203125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"95.203125\" xlink:href=\"#mfde8259c35\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(92.021875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 138.603125 143.1 \nL 138.603125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.603125\" xlink:href=\"#mfde8259c35\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(135.421875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 182.003125 143.1 \nL 182.003125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.003125\" xlink:href=\"#mfde8259c35\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(178.821875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 225.403125 143.1 \nL 225.403125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.403125\" xlink:href=\"#mfde8259c35\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(219.040625 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(112.525 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 30.103125 120.45 \nL 225.403125 120.45 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0a9d572467\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0a9d572467\" y=\"120.45\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 124.249219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 30.103125 75.15 \nL 225.403125 75.15 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0a9d572467\" y=\"75.15\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 78.949219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 30.103125 29.85 \nL 225.403125 29.85 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0a9d572467\" y=\"29.85\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 33.649219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 30.103125 33.354329 \nL 51.803125 81.887847 \nL 73.503125 92.321994 \nL 95.203125 97.420631 \nL 116.903125 101.227847 \nL 138.603125 103.823441 \nL 160.303125 105.732195 \nL 182.003125 107.212241 \nL 203.703125 108.700064 \nL 225.403125 109.779319 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 30.103125 41.344875 \nL 51.803125 26.8602 \nL 73.503125 23.911925 \nL 95.203125 22.613325 \nL 116.903125 21.450625 \nL 138.603125 20.66165 \nL 160.303125 20.227525 \nL 182.003125 19.676375 \nL 203.703125 19.404575 \nL 225.403125 18.8723 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pb9bd8b337c)\" d=\"M 30.103125 31.8885 \nL 51.803125 26.6337 \nL 73.503125 26.5884 \nL 95.203125 24.5499 \nL 116.903125 24.2328 \nL 138.603125 23.80245 \nL 160.303125 22.9191 \nL 182.003125 22.85115 \nL 203.703125 23.53065 \nL 225.403125 22.4661 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 143.1 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 143.1 \nL 225.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 143.1 \nL 225.403125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 140.634375 98.667187 \nL 218.403125 98.667187 \nQ 220.403125 98.667187 220.403125 96.667187 \nL 220.403125 53.632812 \nQ 220.403125 51.632812 218.403125 51.632812 \nL 140.634375 51.632812 \nQ 138.634375 51.632812 138.634375 53.632812 \nL 138.634375 96.667187 \nQ 138.634375 98.667187 140.634375 98.667187 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 142.634375 59.73125 \nL 162.634375 59.73125 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_10\">\n     <!-- train loss -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(170.634375 63.23125)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 142.634375 74.409375 \nL 162.634375 74.409375 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_11\">\n     <!-- train acc -->\n     <g transform=\"translate(170.634375 77.909375)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 142.634375 89.0875 \nL 162.634375 89.0875 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_12\">\n     <!-- test acc -->\n     <g transform=\"translate(170.634375 92.5875)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"223.828125\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"285.107422\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"340.087891\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb9bd8b337c\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_model(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 50,
        "id": "PF-zAtq_eD9P"
      },
      "source": [
        "### Making predictions\n",
        "\n",
        "Now that training is complete,\n",
        "our model is ready to **classify some images**.\n",
        "Given a series of images,\n",
        "we will compare their actual labels\n",
        "(first line of text output)\n",
        "and the predictions from the model\n",
        "(second line of text output).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 51,
        "tab": [
          "tensorflow"
        ],
        "id": "apk95DxKeD9P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "91128b8b-f63e-4afc-81a6-986874adfd97"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x108 with 8 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"117.689543pt\" version=\"1.1\" viewBox=\"0 0 687.5 117.689543\" width=\"687.5pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 117.689543 \nL 687.5 117.689543 \nL 687.5 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 106.989543 \nL 81.934043 106.989543 \nL 81.934043 35.7555 \nL 10.7 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd6f595b97d)\">\n    <image height=\"72\" id=\"image2398885cfb\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"10.7\" xlink:href=\"data:image/png;base64,\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\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 106.989543 \nL 10.7 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 81.934043 106.989543 \nL 81.934043 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 106.989543 \nL 81.934043 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 35.7555 \nL 81.934043 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- ankle boot -->\n    <defs>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(14.593896 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- ankle boot -->\n    <g transform=\"translate(14.593896 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 96.180851 106.989543 \nL 167.414894 106.989543 \nL 167.414894 35.7555 \nL 96.180851 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p7aa4499b7b)\">\n    <image height=\"72\" id=\"image8ff8251839\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"96.180851\" xlink:href=\"data:image/png;base64,\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\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 96.180851 106.989543 \nL 96.180851 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 167.414894 106.989543 \nL 167.414894 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 96.180851 106.989543 \nL 167.414894 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 96.180851 35.7555 \nL 167.414894 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- pullover -->\n    <defs>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n    </defs>\n    <g transform=\"translate(107.471622 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- pullover -->\n    <g transform=\"translate(107.471622 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 181.661702 106.989543 \nL 252.895745 106.989543 \nL 252.895745 35.7555 \nL 181.661702 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p889f3be01c)\">\n    <image height=\"72\" id=\"imageae0ef175b8\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"181.661702\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAEgAAABICAYAAABV7bNHAAAABHNCSVQICAgIfAhkiAAAD3hJREFUeJzdnFuPJMlVx38nIiMrq6qr7z3TM3uZXWu97C62QH4wAiFLICNLCCEe/CGQ+EIg8UmQQIgHIzA3AcY28np3ZvY6M32tW2ZGRhw/RFZV33a82q1s1BxpVN3TlZc4eS7/8z8nUr4vP1RuScxoxPQP3+X0rYxqR2neKLFZwP50g+3/jRTHDf1//YDw4mh5jN3cpP7OW8wPHCfvWPrffcGoVzOtc+rGMn66yf1/FIqjhv77R4RffLDee17r2b6MCKiAGhCjSPuzWlArYO2NxyCCChgBEcWaiBEFk86HSCe3m3Vy1i8QyR3nr2VM3q0Z7sz53Vc+ZDOb8+PtR3zy1jbyccHr1WsUwwF6PiYcHUO/4PxRj/HrQv3NOX/0ys/YdxNmMaeMjr/J3mHy5IDQE3rPB0lRuj6nuGUF5UxeV77/rZ/y7vBTfrj5n+yajNODf2AcDX919D3+/qPvsmv3KR5ncHyK9AvGbwj63pjvvf4hf773I/ZNjicQVRnZkr989AeEXsbm42LtC+pGQQtzv/okjSHmykE+ZjebUIjgxFJIwEvEEpEGbBUghOW5olVcFulbTyFCTzIsgifgJHSyhIWsX0EiiLVoVNBw7W86CPzm4GP27ISjIIxjxdNmk8+aLf7t+DW2Hnvczz5GZ3OI6XhRiFGoQsY0Kj2pmMWABz73m9ipJZuBqePal9OZi4kRVK/EAxEkDxxmpxTimWnGTOFZGPHU73I8HXD/uCZ8/mx1THu8qhARPILXSKkwU8u06WEaMB4k3CEFJQtKi7Obm8jeDvUrO/Q3KrbNHIviJBIQLF8cVDWzNAPl4eaE/XxCUKHUyEIVVcywpZDNFePjS8701aQTBSXlrJ6m7O8yee8e00PLazvPeWhrAuAVSjU4aXASUtq+KtYStht+e+8j3ihe4DFUGqk1IZR5cGRTyMeKVH7tClo/DtLLygHQnqPeNPiRsOEqrAgLtGNFmcUeJ82QurHItcAuYLUN0B6AwArzRBVEU5xi/R52O2ne7w05/aahuhd4NDimbpXgJFnRj8Zv8XdP3qb6YISZnF5fpwpNNHhNarXoUkmZCaiFaAG7frDYDZJWvRSc/chRPvT0Dycc9s7wmh72wop+MT5g9nRE/3ODlNXlc8X05YvKgWR5i8+ExKUTNH0rpUYzMAz2ZzzaPWHXTolAaPXn1XBWFeSnBjdWpLkCDVoX65nmEuYJKu0ClGghZq2S1iy34mLljuFPvvETvjv8JQfZOVPNcEScRLwanh1vsvOB0n8R0Hl5+WARTBbZyWYUxl9fgAmEvtL0hZhnrFtFt6Kg6ODV/IRXshOcBKIKSHKzgBBqi5sp2TxA01w7XgwUxuOkwVzJUysXg7Vrh9uqxZRUXKpLv0vAYyhVmWqOzi3FkcedVeAvK0iNoVfUPMpfsG2nxFYLIxMpRBhlyeIkpuusW25FQdIG2VIdRuKlhcxiDzszuOMpZjJHr1qQFQY9z2vuiLzNbwFhZAxbps+WnafvdcRqdaogcTliDdEJPbPCMbDKSrVaxAumrJGyJobrxaeIYtGbgSQtBlKuY6g1SHe1WJZh93agX1Bvwv3sjF07Yxbd0tWmmnMcNnBjgY8/J1QVWtdXTiRYEym+oGqPyEo5cf0K6i7Ni4Gihw4KYp6C7GKRASEgeM0oo8M0EGcztKpuJLuMpLrNoNeCNJDc6665mOSO5t4W1V5BvR3ZNjMKUQppqLHkBAozo3QOvYFlXYiKkNvASBQvyjRef6bLUqMD6U5BRY/5gz7TQ4vuVmzbGQMRphLINTAwngPTEDgmupecyEA/8+zaHmexZsoKJAIEbUlpVUTXb0gdupjQFAa/IWS95hKlYSQSVRircBr7KUW/7CZFMRgMLNP85WtpJxgIOrWggukDw+TNwKO9MyyKV8UKFASOw4B/Lg95v7yHm7z8XCkGJT9cxK/YMgZWYjKg9t+6pbs0bw1+BGan5qA/WaZoS0rZU815Uu3xeLaLqVoO6QtkEZgtklD4LUqHCrL4kXKwe85hcQ6AJ/l0IYEy5nw42+PpeJus/HKRI6DUbVQIJEBpaHtjtm2arVk6i0HaczR7nt+595j3Bp8QVShVsAIDgXEs+PnxPT59to2bfjkFeU3kWqluyRlZiUTbNh7vkoIQQVyqwgemuvbnWjNKn6GlxTQvUdANHE9UQ7iAl7p0uu5Ie2PIC8+bvWfsZSkKBxUCihE4aYZMTgZkxxnZrLlG0y7bR0ZS/QZYEZw0y58hlSzSgPGKdICku4tBBvK84V42ZmAqjCgRIZJo1nEokInFjQVb3ZDnxYC1qL1s5Fc7IEENEgTTKBL0bnQ1ABChcA0Hdgwk5jC2rR4nSmE8miuxp2h23UnECCKJN7pYpIYrDmUloiaxil0kuO6CtDHsD6a86+C+rVu6I1tedMOW2JHHj5TQM8liLooYcO5S4A2qeM3wunquTgLRQcwFzda/nO6CtIHcBAYmJ2/jRUCWZYKTgLEBdXpzLWYEsQas3EhzhEsd2zRCc2dJe1gg4NXlhqZiezRHtmqa3vXbMNtb6KsPmN3LKaynUk+pUKqjVsuC/EgWpETXTZq/FUYxqBIwRDXLQD0wFfeGE0qfEXrF5QNEYDSkOhxS7hj61uM1UKqhVrvMYJDqOs00dTVuiGVfV27FglIndUWXBoVcAtv5nFFREW94TFr0qHYd9aawYav2PErEENSsgGJXRFArtzZAtehpRRVmWEZmznc2n7DpSn60cZimQRarFkN1OOT4HUP5oOEb/ef0xGHb8iLe4uTgrShosZxly1gNTgL33RnjUNzIB4W+xW9FzMgzMvPV/6vB/jp+ZI1yaxZk0VUfDKEQzxvuOQAhv/79kAthGBn060sNw9tUDtzyjCKsuhkjU/ONrGZkPib2Ln9HjBB6gh3VbA/nFOKJxCVINF2McXyB3P4YMCzTvRODIyHhq0AxWiFzDUXWkEu4hHuuxSDTjhLfZRy0kEUnI6rgsBQSUZtIfsmyJdgLBdzbmvD6xgmFeDwBrymDXZSe8WgvEgoluv8HCgoIHrt0F7sYKrf20hB5zGC7mLPrpjhpiF/QFLREsIp2BBQ7VVAdLZX6xEW3eCW0VrCKJ9BsRPSNh9iHh5heD8TQDIRvbX7Ct4cfcWDn9CTDtFkwnSNdw0mDyROaTjNC611Sd0E6QhUyzmLNglE1EomYtuC0QEMugtmtOf+NLYqjIb3zMXo+wY/gT7f+nUfZnC2Tp9lo8S0aX0lhPL3CMx+4u+ViokqIhlr1Ws65SFkYIO95qk2DH1nI0jOLVhmZmpHJsCK8bH5VFm2fu0R3oMq4ynncDDiKPYwoORFDXHY2IGWyd+59zsm3lZO3MxgNE/luYCABg+Es1nwa5oyjw0mgMH45jhjV4H0agJCw/rKjOwUFpfKOz5ptxjEVo0b0GtBzWN7b/IzBo3PmhxEtEihSSUOeTizTqBwHR6lZOzLcLG88YIjRII382gbkV5FOg3TTGKYxx2u2tBpLUlJAqLSh0oZ9N+aVrTPCKKBulcksEFkNjS/w08UCNSy7htytASqJkcZnvGg2KYzHoFhRjCQ3CxhOY8QJfLt4yuBBzV/PB8T+4NpTC8iyH5ZLwEmznJCNGGIQbCOJtL9K/n9N6dSCYhBmCwuShQWtFpD2XcDIlLzujtgp5jQbDrMx5AKrSlAh6v8J6O9QQU2AseM/zl7lo3oXSDFl4WJRDePoGGvGUBoeZSd8//5P+eDPDB/8xTs8+K3PsCKU2hCX/XizxEEe8BowRLK8IfRjSvN3BQdJE7Bjw+OzXR72zwBwJF5oQXtMNSfXwH0758Bm/PHGf+N/P+OzepPfG/0CgEojXjMCK+VYUYKm+GRF6fUafB6JHZD2HQLFSFYK59OCU98HErPo1VK3Ff2SRAO8pnbQw/wEZxpGdk6pSRHLfvwFN/Mk67JE8qyBPKI37Xf9mtKdgnxD8RzGowHvb+3jH6TpnlId49hnaCpGzHESKNVyHAMgfKd4QtQUlJ82AwJpj0aiWVcKKtUyjg1GIgfDKZV3NL0biKWvKd0pKASyuZJNhVmVbtzQFqvtghdpPyCUanES2TUpQz2PGadhcI3aWOCoqELVpvUNV9Fz/qWjfF9VuuvNNw2984h/YZlME1B0YigkjQMveuyLXlkKuoay7YGN48oaFtXXgrBHYgKNmoJ9bgKFa6g7SDndWVDtKY480QpnE4dBKcQxFM/AVJf2fi2zE4YyuEtWcxN7GNRQYynVEjAMs5oia6g6sKDuWs+qmDLgZhHxqR9mMDeWGy+Tqy52U0ejUUOI5m4haa1r3Odn2PmA7HwL32YgR6QQj0WpSZNPti1gISblaetOy/LctOhbWheDnMCgddPn5QYnsz6965uBvrZ0GqR1OsPEiC23L1Eci1pqgY4XFrWs8lslBS4wjFwg2tRgJJJLJGjaLu4bS7+Dar7bXc/zEo2KqVccUC5xGaAvTmnAqiV0NaVDikVWUlFqJTKUhi0j5BKYeUddpYn9dUuHQDEQxmOYTLF1shaDtEOczVJh4YoiFspZoeaWQ2rLE4tiiIxMYMcMMBI5nxeEaYbxd4kPgrTvIgYkttlqOc7b8tPtQMOvk0vudUFsW3eFYKAjPuh2GocRTuOAs3hEJBFnMRpOwwCvGcMLaT+XQCCCZngM/sJkwwISXMyC49BnPu6RnVtsecOs49eUW9tQN409Zrrazl1jGcf+stNqTHIjJ8npvGZLq1mQ/AtM5DQsuxqlOigt2Uywd+ndHRfFeOVxvc9hdsaemTOSBmvmxOyEWi1DUzG8ABwD0k6nVUsQCav45KShaL2tig47M3fs5SZXXo/jZvBPJ2/iY8YPRv/F2y4nEnmUzQDabSrpdTfHIeARClEGIlgE036WGhjHtN9jZNKtHzdD8mND/7liJ/Ud2u1z8SJeeTYb8VG1wzgWaRDhUq99UYAqpRpmMaNUwasS2iWbC7caNNEjlXrOm4KsBDdXTL3+PH8rLrbxScWTHx/ydPeAJ+/t8IOD/+G4GfL+bJ86Zmy5ORu2YhJ6PJ7sUjaOIvMMsprMRLZcSc80VDFjHhzOBF4pTtl3Y/728dvs/7xh8OE5PDte6+u5oCsFXblJ9/Ep9//lgHLb8hP7KlXIeDEZcvrpJuIFHQbcoKapMuQox1aSBjNzBQtaBMRFiIIGQawy2p6xvzGl+uUmw599Rnj/w7UrB24tiykSFnvb05amsGjVxPSGAY0GjYIJCc9IFCSAirbfEXTx2R4aokl76ULsRDkAvwLs5tDCzGtePgAAAABJRU5ErkJggg==\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 181.661702 106.989543 \nL 181.661702 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 252.895745 106.989543 \nL 252.895745 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 181.661702 106.989543 \nL 252.895745 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 181.661702 35.7555 \nL 252.895745 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- trouser -->\n    <defs>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n    </defs>\n    <g transform=\"translate(195.837161 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(195.837161 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 267.142553 106.989543 \nL 338.376596 106.989543 \nL 338.376596 35.7555 \nL 267.142553 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p099880c7f2)\">\n    <image height=\"72\" id=\"imagead52dbadb0\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"267.142553\" xlink:href=\"data:image/png;base64,\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\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 267.142553 106.989543 \nL 267.142553 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 338.376596 106.989543 \nL 338.376596 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 267.142553 106.989543 \nL 338.376596 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 267.142553 35.7555 \nL 338.376596 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_4\">\n    <!-- trouser -->\n    <g transform=\"translate(281.318012 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(281.318012 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 352.623404 106.989543 \nL 423.857447 106.989543 \nL 423.857447 35.7555 \nL 352.623404 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p0793235477)\">\n    <image height=\"72\" id=\"image116132c01d\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"352.623404\" xlink:href=\"data:image/png;base64,\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\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 352.623404 106.989543 \nL 352.623404 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 423.857447 106.989543 \nL 423.857447 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 352.623404 106.989543 \nL 423.857447 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 352.623404 35.7555 \nL 423.857447 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- shirt -->\n    <defs>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n    </defs>\n    <g transform=\"translate(374.826676 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"143.261719\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"184.375\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- shirt -->\n    <g transform=\"translate(374.826676 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"143.261719\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"184.375\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 438.104255 106.989543 \nL 509.338298 106.989543 \nL 509.338298 35.7555 \nL 438.104255 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pcd0153ca52)\">\n    <image height=\"72\" id=\"imagefdc7f9e692\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"438.104255\" xlink:href=\"data:image/png;base64,\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\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 438.104255 106.989543 \nL 438.104255 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 509.338298 106.989543 \nL 509.338298 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 438.104255 106.989543 \nL 509.338298 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 438.104255 35.7555 \nL 509.338298 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_6\">\n    <!-- trouser -->\n    <g transform=\"translate(452.279714 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(452.279714 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 523.585106 106.989543 \nL 594.819149 106.989543 \nL 594.819149 35.7555 \nL 523.585106 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pb0401eaefc)\">\n    <image height=\"72\" id=\"imagee1540fc578\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"523.585106\" xlink:href=\"data:image/png;base64,\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\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 523.585106 106.989543 \nL 523.585106 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 594.819149 106.989543 \nL 594.819149 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 523.585106 106.989543 \nL 594.819149 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 523.585106 35.7555 \nL 594.819149 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- coat -->\n    <defs>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n    </defs>\n    <g transform=\"translate(546.202753 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- coat -->\n    <g transform=\"translate(546.202753 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 609.065957 106.989543 \nL 680.3 106.989543 \nL 680.3 35.7555 \nL 609.065957 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p16413eb098)\">\n    <image height=\"72\" id=\"image262a420604\" transform=\"scale(1 -1)translate(0 -72)\" width=\"72\" x=\"609.065957\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAEgAAABICAYAAABV7bNHAAAABHNCSVQICAgIfAhkiAAAHx5JREFUeJy9nMuvJVl21n9rP+JxnveZz6rK7urqdptyt4E22IARNjBARmKAjJAsBowZMUSMGPAHeOA/AJkpAxskJiAZZGObtt3Y7kJd1d2VmVWVla97877OI05E7L0Xgx3nZpaxq25WZTmkUOY9J06c2Guvvda3vvXtI/9Qfll5icPdeZ2L79yinRmWt4XNjUj90HLnPz0m/ujup37WjEZ0P/eTXNwpqM4Tk7sLpI+cv73LxVcMpofiTLGdUp4nypOWVFhWNwv6iTC/21H83g9I6zWmqpCiQEMgNQ2ofuJ75PYNdFzR3ByzvOUwEaqziG0S1aMl+oO7aN995njNyxgHABFUQGX49KfdQeT5efk3MHxeRYYzv63De8/PfL0ann/n9n7GgMn/F2uff8eL370d4eU9/sw1Vzjcla8cbp5mIy7uWNo9pX9jw52bz7hvr6PjKj/PdIqZTcE74v6UVHns+QZzfAp1xfmbBWffgPLEonaKbRPNgaGbKiYKyQsmCP1YaGcVsRCWrwv9VBEtuPbhDVzTorMxaVSQvCHWeRjV3WPCvQ+QsqS/NqWfe86/4ll8NWGCEB5YiguD29Q4767kQS9hIIMYIezWLN+MlNfX/OLr9/ilvT/l1/hFwmQ3O9RsSry1Txh7zr9a0s2F8aOK+XuGOPKcvwU3vvWER0dzzqhxjcnGnimiikyBJNgW3NoQaiV+bc3efMVZPGD+/g62CbQHJd3U0E2F5lqevJsc4O5/iFQlzfWSZt+weDNx/e2nrNuCpexSPRPKC4+z9krDvrKBTOGRoqC3BglC31ua6OnVEpPJNxJB65J2rySMLKEWUgHJg1pBjaAGvI1Yl/J7Mbu/DCFEDSB6eS0GjCjexrxMDGCE5IRk87+xyh+OlaGoayh8XqIACkmFpH/O8hL5ROz63AYS5zA3rpHmY2JlGD0w9Bc1f2hex4jy8dEOX9/0qBjaN/Z4+p2C5IZBK0Q/xAwRJMKqK/IDV0oOEGCbIbZ5RQUkgQmgvdAFw6Z3SMrGye8rJiiIEAtQp2x2LaM7t4lVgW2VYqEUp4YnT3bQzlBfCG4Npr16XrqaB4lBRxX9bo1aoTxXTC9cnFU82NshLT0SWhTo5o7mZgQD7sJgW0Etl4FYFELMUVctJK+gYHpQC9Flm6EgMZ8aDSFaeGFckl7wOqeoV0JtiNMK9RZJimsV1wBLh+kF24HpFRNftYGMkGpPP8mX18eJohCam5bTdY3ZGEgJMYJrEv7coQbcWjA9uI0ifUJswjbCclURG4tLgyGUIU0pkgQkB2yJIAE0CjEZJIDpEyYkJGQPMp1iN4L2+bu2hxoh2cETN4LEnBGTy4lAqgoTI2nT5nX+RQwk1hImBZtdS3kRmdxbgCqrm7ssbtb4lSBRUcAvekaPPcnmJSJJ8WvFtD2iSrGAi7MS6QwmPjeO6BAOEggyGAPUAb2h7y22E8wmYpoeO/KkQnAb8BfZG22bEIU0GEKtYAK4lcn3NxBLCLVBJqM89ymRNl/QQADRG2IJyWfsIgACYvQ5xhADSTF9fkpJmpdCVIgJQsJ0YDY50JNAklwuJbEyBOjny0sCSBBCb3EBTB+RPmZP6k1eMr2gSo5JKQGW5CAWQnLkewokK2iZX9fCI2WBrj4dCl45SLe7jvV1oZtZ2ukMgMVbkW/cfsp73U3SyGOsQWLCtiDuBWTbKbLeINYwejonVhY1XAby8kSpzpTooZ0bkgfX5PgRW6geW/plxfiRYp+coU2DByTUSCroR4I68MuEaXriqGB109LuQBwpYZQuASOA6S3jhzPceYnZbGC9/mIGwgj92NDtKGECYZxdur6x5Kd3P+aj0x1SWWCsRbYzqUOaFjBR0S6DsvI05GBaQjfNT1ydKeMHDam0mFAQS8H0ueSwvRBPBdsK1WkgnZyibYutKpwIGKFYZo/x64BsOjBj2h1or0XUKRQJMYotIsYmurMx3Y5DVLFl+alDv/ISkwQm5rhgurye203B03ZK1/p8jbXQ9FSnkVgYQpVxigSFmNe57RO2VaIXYjXEhfzxXHYYSDYH2e0SiWX2tuQEqUpQRauCVDmSN5fPJ32CZoN02WPU5QxJa1CBkASxibLLMY7EsCS/qIGGuGJag91Aca4g0JwUvLt3jXBRZMuVJeZ8yeiHHVoXNLen9BODbWOOQZqwy47y3NGPHO2ukgqlfpprKjU5xqUCQiXEOie35AGTX5PJGPGeOB0RZiWhyl4rEeyqIx6fYA93SHaUPWdlcUOciYUBA34JtkuYPr4aoAjDDMXnXgRgOmHTeSQOgbsYPKnrwWUor1ZQJxndJgWTZ1MtpDqhZSKWnuRNPl02UqwgjPRymapALAUtC0SEVDtCbYmlIXqGLPW8gMXkBIKCbJ+XHBokDIkjKfqqDGSi5uXVgm/0Ev2umgISNIce018jjB1hZOjGhvO3DO1eonpaM3vtTUSh2Tf0E2huRb79rfvslmv+R/omSEUsoN1XYplIs0C9s8EYxZqEEeWk2sFtDjG9srph6ebZu2Kdn6edzZhf/zbrQ0c/T9giEsVhO7mcZLWKbRXTpbwk41+c4q9sIFXN3hPAdoprcmFpG0fTeEwU2h0BStq5od3N1bd9+5yfOnjG+8f7HO1PkSjEWY/Uka/eOuZfv/bfOLQrnjRT3u1eR73i5y11Gbg9P+cndx5TmsDIdHiJ/Ib7NqeLQ0yA5mbA7nR4H9mpW5LC6WiXbu4JY4VZjy8CUcrsMQPGQgTbKRIS8sqWWErYLmFbi2vAryIkxfZuyFZKLA2hNqDgl7nGWp7W3DN7rI9HjI4NkqDvHLGyfOj2+K3Dn+TALbl7tE/1NGeirjX0ZeJe61j2Bd4kRr7DmcTx0YzZs1wuJOfoesPGJzZDJvJLk8FpEDQYQsh4KwPWwYNkSDR9RGIexxc3UIz4i0B9bClPI9W9E1DFf/1GnhUL7W6OEfWRMr/fEQtDce7ppztcf5yY/2iB9IkwLwm1ZXm75j8e/zxaKHvfsxx+74LkLe1+SayEdjpiORujFo7KHLMOP1D2//AI+kB/e4d2x6NWiIW7xFXRKxKFdmnpU0GxFIqLDFi3xFu5iJhVi6w3pO7TOaGrB+mQMjZpE7LeQIyYQA6gVonlc/Tsz1ucEVJZ060Mk4cd9oMnEALlbEoxqoAZmwNPLGD6oMfce4j1HrvaJVWeclrQzR3JQqhy8J4+aNEPPka7jiIm7MUErJAKixqh3S/opoZYSfaimGswidlApIGQDECI+XwldEddc/JXRpx+SymflcxvvIFEOPl24jvfuM9pO+JefUC/9NTHBtkExMpQOQ/3cC6n8qog1Z5YG0INsVT6iUVmU7Qu6Q7HhNpmYDrJ2SgWGQKEyuJFwFp0VBF2yqHq15y1hmUUS6i/suDN/Wf8cPeQ0+k412iVolZpPnBI3Ke46PEpkb4okpaq5PRt5R///B/x/dNb3L95HQnCz/y1H/Nvb/9Xfthf49fd3+bj8znd/T2k7TL3E4aZA/AOjCFVBbH29LUQxolUKe3UEPcmxNrTHHr6kRBGQhg9XxYIhJHJYFSVOC5odzymV9w6IqqXFEms4B/d+QH/bPcP+O+7b/M/97+OQXltfMaOX/Nfdn6Ks+WE6sSydzyBR1/QgwAkCk0s6KJFeoPphGebMe/3hzzpdwjJIKKZ+ykLsEIYWfpa8CNLMa4hJdLIZ7axkgEHRWJlSZUnVvn1WHJ5bllENZqB4qiGvieOPKESrAVJOQGk4jn3dBFqHsc5D9sdjpdjZGAlm+gJrcveHXg1S0z7ntEj4bfe/zr6pGLvHcG1yoPuNv/m6T9FTMK5hKpABeuvzEiFcPampdtR2j1PP95FohJqIZTC6rYwvX3BpGo5fnKd5XFFLIXmcACJYyVMEmoVrIJRVouC2VdvYEJi8XrJ+nrmgPxqMNC2HFH4rfe/zh8f3ebZ/V2mP7Ig8P7OPrFSZg+FnbsdbtEjF8svbiBiorhQlkcV9VPD5FGPW0f6UcmSmlgpm4MO6xPGKe3cEAuh21W63QhqsY1gohBLIXnoZ4k3pgt2qzWPpod0E0Ms87KKlRLGCSYBMYrzETFKN/V0OwUmKt1U6KdDXWgzZcJ2OQLpuOT4omDygWX3h30ujfYdoYbRcaR8ukaaDt1sXoWBIn6llKeG4gLcOmLXPcWioDg19DMINxP1qKWVGrfJwdmuDbY0uLXgmswASsgkmFsLJ82IoAa7NplUCxBGOQMhht5aMNAXBrGJaiO4JnNBfu3pt4zlgOz7qeTAXytaJaRIxMrRT7a1WIYLANIFpO1I8RUUqxoj1Ulg9MhTnSr+bIO0PfVRgRrPOhnERw4nKz4yOxSLSLJCcWHAGIoFlBcZJqQiU6Hd1HJ6MWLdeopzoTwLpFJIzhKr7G0km3nqWlBn8Evyd/eR8rwklpmGtR0gGYu1BxEtFDfpcT7STwraeV6CsRoINAVZb9BVA33/qWN/qc6qDh3KVFi0zCBNhsZE4QPzoskzlPSyws6M4OA9MZ/mkowXYjQDlZK9a8tAbovj/K9k5nFLT6R06Y0mDp9NA7PpcswSk3LScEoscuDftqAyy3i1LuvV0rx3NIeO1euwOTA0e1NM0Ey/WmgOlV+4+SF/Y3aP/3NwhzDJHQgTFLfJTUAJuZugMdOqmYMeikjlEvKrG7KRZDqWNHA6yifo20s694UVog7wivhcU8RoiHVic82gkvtnySuusUx3JpkBbVv4lDh0tRhkLe3M0B30SDD0s2HWO8V0QtiJ/NXph/zd+sf86mxDKCd5VmMOonZotch2podeGAxG0ucDVTPEiaE3pkN7RwcjbT2IrXG29xn4JPG5e6KaOW+KRDdP+b6jiBSR7qgijj3Sl4j/dBNcOUi7RrELO8xqfrDLQrATnvYzPgpzQm8vB1cuFF0pxSLiz1okKbawqDeUMwtnnq60TC4Uv+gw0eFXueA1rWA9qBP6IKQid0dk00HXU5x3qJVLnjkWAlhcGUgqhMZBFKTLBasYJbUGDdmjbRszb/VK6I6YGB1Hugcut03Geb3bDtwK/MLw/fNbGJR+7YeKWakfbbCrFlm3sFjl+s1axBjm4Qab/RGhskw/7DEfPsGOR4z8PmFih/ijxMKw2TWEWhg96dFnp6S2xceIf1yj3pFGJXHsgRH7O0su1hXtwwq/NKjRIXMJtsneWp0o5myFLFa5L/ZFDURKmC7ldEouA+A5tJcgnLYjjropxKFv7iQ3+NqA9AGNW9pVM/vYxxxkQw6yOdboZbtakua4JdtYo5edVAAN+b4YyWUGgCiljZlQ7AS72TYKh7eHPpztUn6mvn9FjGKM+Iue6sTS7hjavRfwRFLcWrj/wSEPT+YQhfO3BNNZlren2M0E20KxSkMXNT/Q+R3H6mfXVFXPo9Gc/dmbhEpY3TTEajCC5KzU7SW0inTzgpvN1zBtZHm9ypV7kScsFtAeBta9p1kXjI+F6kTpx0I/4XLZo7nTq6sGXa3RPnxxA2lM2EVL9cyRvIeB280GAr+C6qOCWHlkkujutKhCs7E5Tm0MfmU/EVibr/T8y7e/y2vFCb8qf59jOycWEK512CpghvKlcJG3ds7YK1f8jvs6p0cVtofla0K7l0he0TqAU/yoo+0dqXFUJ8roSWCzZzNXPTg9gG0Sul6TPgNFX9lAANJ22KbEbRymNagofgXlhRKq3ONSIwOBrwjygmJsAEufsDq0ybFOJV3nsG0eQWgNEYf6CCoYk0flP6FWGFJ+EIwoMeR7G6NMqpZFWZO8y82CreeQ46LE3KL+rCL1pQykMcLpOb7rgT3qgxGxFGb3A+N75/S7NbGo6XZyrRXSVmM3DCYK0vMJRYa0hrvrA571Y7pnFfuPlVSA6R2x0lwijBOhcjQzTxpaG1kWo/hltn5yQmxynHEHiZ/ef0hMhtX8Gm6dl2CWykD9LOHWieJ49ZlL66UMhCa0zdSkbabYFkDxq4CcLXDOYPr6BXJ8i20GRi89bzxeGi0IZ2093DPXYqnPxarE7BFqDUmUPlr6ZNEBTZuYMZhthyJVBx2AKIfFgnm5YVEMRJsddAOa2VC3CkgbuJr/vIS6w+zMSfMJi7emHH8noeNIN6/Ynb/GZm45+yaEecAuLNUHRVasXoBtM34pz8PQVcj39EvHe5NbmCowvW+Y3l+RKgtaEmrQk9znipXh4/V1PprvM/5hwfz9NXYTqAZuO3nJFEolPLuTo/u02NDcjMTSEkZKnAakF5pDj196dn7sqB88eoUaRWtJuxPaGxPO37T8vb/5fb45fsx/mP0cR+Mp3Sxx+M1jro2XvPMnd9j5nlAsE/XjBrPcIMuGdHYOMWbq1Qjl8RvEekqoPHvvddh37uJnU+AG/cQNjEEgVZbFcUk3LZjf7fHv3COtGsrZhKqqoPCkcU2clTz76eyR++WK0e0lzW7B4d6Ct/ceswgl379xi/NFicSC0e8XsFq9IgO9cKiBwgRK0xODwbdgN8LFusoNvm5oITshlhZCgekDYrMEj8IjzpEKmwtHnzupUhZQeGJpiaUg0WBC7rbG4Tp1mY8W7xDvoSxy0Vz7LHzo4d56n+PNhGZVkFaes6LmQblDGx19bzO6/oxWz8sbKCnSR2wbsQ18vN7BisK9ETd/f0M/cZw9nXE6mVH1sL6eQdnqRo0JNdXphNHHMyQmwqQglobFG47lNzpMFTm9qPEXb9BPHOdf9YQx2NZiN45YCqvXlDALgGP00Q2k6WhvzWh3PLHIkuFYgG3gt//0J7ALy86Pcrunm014OJ/mibWAgfpZRMOrDNKQK+iQs8GyKznxI8pTobx7RDGuUbtDNzEZSO4wKFcHbbUzuKFF3U8zT90cCNODFeOy43Svot33dGNDtwP9JAdg0w1ChoOeatbSPZkSpiWmsGz2Pc2eIRUZCG5Ln+qRpziH+fstxemGMC3pp45YCJu9XLL4ZfpMVcdLGUhjRBYrHDA6qvjg430ej6dMzgblWEyX2emykFUu1V2xEGJhMEGJW1mwy5V8TOa5ymz72Re1i5Ir9KroWZW5g4ta0lCo6ovnQO7HUljf8PRTdymGACgulPJcKc66zyxSX8pApEh4coQcnzCdVkx+MCeMSsaPcz1DKi/rHBmEmFnKkgvFVOaua3IQShmYPSUlIURzqbYwLxJkW0xoFFcE5vWGi0qJtc2iUv+84wHZkFuqJIyUxRtbSiZ7lm2Unbst/tkac7okvvIlliKaIqaNw4Dyw0hdkeoi63qsPN8PseVx4Dn2+TOxcetBl8h4S4K96Inp/0fgJH2uYUzPX9/yRyoQ6lzcGjtIXgYMJm2AENArBuqXzmJpUtBcU8JO5KR3dONbJCd08+dVs+22MysYm8UM5XlEkuZrAdcIbedQFVwLrgmoddg2ixhsB3aTjbVZF5xWNW4pFGcddtWDEdzG0k0Muq3Y22zQUMPmZkSrCMEgneAvDOPHHressF2PGEGvEIZe3kDeEnYi5V7Duh+RvL0UREEelFsPeFDz664hI9ik2LHF+ky0pd4QgKIH0wSMN5mlLCUvjVYzKdYa2s5hN4JbtMhqQ+EMpveAp5vnZ7BtlueoE5gEJvOGvreEYOl8QT82xMphvAO5Gh3/8jjICljF+0inmZQ3KT+cpEHJ3gOStyCoBb9W7LpDouJXW5G5RRtHiJlnsqsOtQa/KkA0b2bZZKmfaQxd46kTxHGBFSGMPaHK3sawbcGvlGKRZRzrM88KMv9t9IWSZ+Bvr+I+n8tARjBVYFx2rDQLud1KmTyMuHUkFVldoZIV7WqgPuqwT84gRkrAtiWxLPFnluQN9bMET47x6wnjqadf2exJfdYklc8srRSYCJvDEtMVhJHJcc9lzpsOxo966o8uqA/H9NOa9rwkTBJxFjNs6PM9JVwtg30uAzHoxWXoTGwV8W4d8YuOMPaosZfZRQ2YLuVsF7MIXLrcI5NeEDLzSAjQB0yXMF2GBJdnBNPnAjj551sK4jAB+YsG+XHXY9qIbRRbCakQ4lYGkzTjnytSHZ/LQJKU2BnWbYFdZ7Tqm7wUYmkJtaWbPifuUSUVBqkrUCXWnli755W2U7qJgeuHxFEGdf3IDBlvEDMUkJyCCrbN3REdZRVHZhTzxJ0WBf7WjXxfI/jVMKPYHOAXEbPYwKb98rKYJIXO0nYOt5as1uqGnpY3xNrQj3Nqtu12F6FB6xJS5nliaS41P2pyuzkcTIhVzkqher5XLJZCKvUyCZg+6wuTzWVIqKGb5ffbvYwCbCvUT8Gtt0Sd4Jfgln3uqLbdlxeDpE+YlaX1JdMm05eQN7OpEdqZYXMw7AFrs2BbxWG6KaJKc1DQTfI14bBDfKLdrWh3s8K+m8klqFSX++n9PKHjQBiVubCVQS48nOqz+PwSZ20h+BACttnNdBFtuyvXYZ/LQO58w+TDMf2zgsmDRHXUEMeeizsV3URYvaFwJ9MIfeOhM7gLy/L1zNVs9pQwi1SHDb/8tXeY2JZfl59FYk0soNvNMmAtE1IHXBG5c3DGYb3kD8KbtHctth1kNINQIYwT6rLQ3XSfBJa2zRmxWOZWTzo7zwzpq6RcP3H0Ab/IadOvE9IFqLPWMNYQJombuwsAzsuarrP0pqSN+avCXsBPO27tnvOd8X2mtuE3Zt9mM6lJXgmTCEXCjgJ13VEXPa9PTrleLvje6A2Sc3kn4rDkXuzH65aYf6GlvWUzba9I21+JJPtCBpKmzU3EqeHsa44nP7OHX8H8/cj0o0hx5jk5vgFk0XkZYXKhjI7zUtzMHWHseXBtxL+/+CVK37P53h6HfxyIhdDsW2LpSGVBLEecl8pvX9/BjnuKd2vm9xrsOlCdlISRoR8ZmoNclPplZi+3PTuES3ahuLBMf1C/7HA/Bw5qGkYPG+xeydO/ZfmVv/O7/Oa9b1H/boH9gx8wunmd/uZuzmIhb3AzZyv04RNQZTadIHVF9/o+R0/mdCXc/qMN/n+/i9QV8xuHpNpnBYk3hNqxeM3RTz07Pw74P7lLWjVUdWYTZTalvzFHrcGdN5jzFTquWd+ZE8aG5QSaNzu6E0ecVl++gQgBaXrc2iG9ox98Xb3BlCXq3dAz3xatyuUPAaSEmKwZyrugyfCgMBTTCVIUxMpdGic5cylkgKG+8wVShGwc73PrucyAU0IJCdLIE6ttR4O8oWUtSJ+uTNZ/bgOl5Qrz4BHF2YTJvTv859vfYnNcM7lh4Dtv0Rx4moNcobsmg7dqv6QaV4gqm72aMLasbliWdxJaKmoKJgdvgmQ6RG2OMcnljXL9JMuFbWOpvnEb00VC5UiFod11LF6zxBIkFpg4Hjqy+Xn9hXLjfwnFImCfnnL1/PU5DaQhEM/OMZuW+tnrXBzVuIWln8D6ekGzZ2j3yQZakzeSCNhNBQqbfZfT/J6Q5j22imw2Ze5+bo/tbiCX+aA0bPvuJ4Z2r8D0ww7nQtjsCs01zVhp8Mgsu8kQozqC2fsrzKpFF58u2HwlBro0VExMP2jpRxUmQLFIWQqjeinfjRVDeZArbxMz2yea29Xlx55UeLK+UGFLsr1AgkkvlKeC3Qjl2YCiBfpR5qLDSMAopIycbZN1i8VCMR1MP+4xp0ukaT9z28GrNVDf4f7oPa6/UyKTMf3r+4RJ3vkW67xRTscRU0SWk+whpssVuunzprz6OBvj/E1D83oPRaIY9TgXidEQgiVeFNTvWqYPspZQhnqsmwmbPUF9zuoSoToWxo8ifp2oHq4xqw1ysSQ+O83Y51O2f79yAwFZwr9eY2PEHMyx3mA3Htvk4jB4Rc0gt9v26YfWsQngmpQDOiBlwhSRquwpfaDtHcYojXOAxYQB55jhF2Psc+PYTQ7GbqA73CpgFmtkuSat1i+NfV6ZgS4N1WwwD55iC8/Bsxk7d2tiaVkfOsLIDnvN8vIoVumyPEGyuDKMlN39BZUP7FYNhQms+pJlX5CS0FwrsH0x7FVLQ88t66nLY2Hv3YBfBvxpTvP0AV0sSSFcuQf/Fx2vxEDad8Sjo/zHxw8xgKsqqjuvkWY1YZw7DAB+EbDrHvU2b0FwllQo1ydLRq5jt2goTeDM1Xhb0wXH+WzCpskFpwnDj5r4XOG7Rpi885j46AmpD6TPsYw+7XglBvrzDo0Js2owqviuwHRFxkNJwQibg4LTn3D0E0VuNYxcR2EiZmDhjSQKE5hVG07vNFzseUb3PdOP4rChzuMXhsnDiDabz9wY93mPL89AfUd8cpRbzsZgrc19+VvXCDs1Z285/sE//y6/MHuX9zY3+WCzT1TBipIQvCRGruewWvIrt7/LbX/Kv/qdf0H9mwt4ckz5fx1iDNp1pPOLl6rQX+b40gwE2Uj6gpBdfO7To7mB+NfH9/nZ8jEXseKDzT4AUYWkhkT+zR8via8UR/yEP8cWEWlawun50N/5crzmxePLMdCfp2JXzan22Sm+adl7r+Lfffef8Gv7i8tdzTlJ5UFvgsu/GQT83uM7WKMU3x+h64d/acaBL9ODXmyrbNm7FInPTuDZCdNRxd7eIc1uxfpmQq63mec2CRHoW4duLNIZyiOLW8P+DwO6XP2lGQc+z6/gveQh5i/YE6Gf7I7qZRdWnqvqX9yC8Ge2HXz+B7raHo3t8f8AoYRACGZhdUkAAAAASUVORK5CYII=\" y=\"-34.989543\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 609.065957 106.989543 \nL 609.065957 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 680.3 106.989543 \nL 680.3 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 609.065957 106.989543 \nL 680.3 106.989543 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 609.065957 35.7555 \nL 680.3 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_8\">\n    <!-- shirt -->\n    <g transform=\"translate(631.269229 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"143.261719\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"184.375\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- shirt -->\n    <g transform=\"translate(631.269229 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"143.261719\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"184.375\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd6f595b97d\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"10.7\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p7aa4499b7b\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"96.180851\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p889f3be01c\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"181.661702\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p099880c7f2\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"267.142553\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p0793235477\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"352.623404\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"pcd0153ca52\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"438.104255\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"pb0401eaefc\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"523.585106\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p16413eb098\">\n   <rect height=\"71.234043\" width=\"71.234043\" x=\"609.065957\" y=\"35.7555\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def predict_image(net, test_iter, n=8):  \n",
        "    \"\"\"Predict labels\"\"\"\n",
        "    for X, y in test_iter:\n",
        "        break\n",
        "    trues = d2l.get_fashion_mnist_labels(y)\n",
        "    preds = d2l.get_fashion_mnist_labels(tf.argmax(net(X), axis=1))\n",
        "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
        "    d2l.show_images(tf.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])\n",
        "\n",
        "predict_image(net, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 52,
        "id": "Zy8hdPRfeD9Q"
      },
      "source": [
        "### Summary of the first approach\n",
        "\n",
        "* With softmax regression, we can train models for multiclass classification.\n",
        "* The training loop of softmax regression is very similar to that in linear regression: retrieve and read data, define models and loss functions, then train models using optimization algorithms. As you will soon find out, most common deep learning models have similar training procedures.\n",
        "\n",
        "---\n",
        "\n",
        "## Second approach: logist regression with `keras.Sequential` model\n",
        "\n",
        "### Initializing model parameters\n",
        "\n",
        "Remember that **the output layer of softmax regression\n",
        "is a fully-connected layer**. Therefore, to implement our model,\n",
        "we just need to add one fully-connected layer\n",
        "with 10 outputs to our `Sequential` model.\n",
        "Again, here, the `Sequential` is not really necessary,\n",
        "but we might as well form the habit since it will be ubiquitous\n",
        "when implementing deep models.\n",
        "Again, we initialize the weights at random\n",
        "with zero mean and standard deviation 0.01.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential()\n",
        "net.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
        "net.add(tf.keras.layers.Dense(10, kernel_initializer=weight_initializer))"
      ],
      "metadata": {
        "id": "8IMeb63B9wmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax implementation revisited\n",
        "\n",
        "In the previous approach, we calculated our model's output\n",
        "and then ran this output through the cross-entropy loss.\n",
        "Mathematically, that is a perfectly reasonable thing to do.\n",
        "However, from a computational perspective,\n",
        "exponentiation can be a source of numerical stability issues.\n",
        "\n",
        "Recall that the softmax function calculates\n",
        "$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$,\n",
        "where $\\hat y_j$ is the $j^\\mathrm{th}$ element of\n",
        "the predicted probability distribution $\\hat{\\mathbf{y}}$\n",
        "and $o_j$ is the $j^\\mathrm{th}$ element of the logits\n",
        "$\\mathbf{o}$.\n",
        "If some of the $o_k$ are very large (i.e., very positive),\n",
        "then $\\exp(o_k)$ might be larger than the largest number\n",
        "we can have for certain data types (i.e., *overflow*).\n",
        "This would make the denominator (and/or numerator) `inf` (infinity)\n",
        "and we wind up encountering either 0, `inf`, or `nan` (not a number) for $\\hat y_j$.\n",
        "In these situations we do not get a well-defined\n",
        "return value for cross-entropy.\n",
        "\n",
        "\n",
        "One trick to get around this is to first subtract $\\max(o_k)$\n",
        "from all $o_k$ before proceeding with the softmax calculation.\n",
        "You can see that this shifting of each $o_k$ by constant factor\n",
        "does not change the return value of softmax:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat y_j & =  \\frac{\\exp(o_j - \\max(o_k))\\exp(\\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))\\exp(\\max(o_k))} \\\\\n",
        "& = \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "After the subtraction and normalization step,\n",
        "it might be possible that some $o_j - \\max(o_k)$ have large negative values and thus that the corresponding $\\exp(o_j - \\max(o_k))$ will take values close to zero.\n",
        "These might be rounded to zero due to finite precision (i.e., *underflow*),\n",
        "making $\\hat y_j$ zero and giving us `-inf` for $\\log(\\hat y_j)$.\n",
        "A few steps down the road in backpropagation,\n",
        "we might find ourselves faced with a screenful\n",
        "of the dreaded `nan` results.\n",
        "\n",
        "Fortunately, we are saved by the fact that\n",
        "even though we are computing exponential functions,\n",
        "we ultimately intend to take their log\n",
        "(when calculating the cross-entropy loss).\n",
        "By combining these two operators\n",
        "softmax and cross-entropy together,\n",
        "we can escape the numerical stability issues\n",
        "that might otherwise plague us during backpropagation.\n",
        "As shown in the equation below, we avoid calculating $\\exp(o_j - \\max(o_k))$\n",
        "and can use instead $o_j - \\max(o_k)$ directly due to the canceling in $\\log(\\exp(\\cdot))$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
        "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
        "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We will want to keep the conventional softmax function handy\n",
        "in case we ever want to evaluate the output probabilities by our model.\n",
        "But instead of passing softmax probabilities into our new loss function,\n",
        "we will just **pass the logits and compute the softmax and its log\n",
        "all at once inside the cross-entropy loss function**."
      ],
      "metadata": {
        "id": "ey8_XB5DEDAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "Ti-A-k8w9wqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization algorithm\n",
        "\n",
        "Here, we **use minibatch stochastic gradient descent**\n",
        "with a learning rate of 0.1 as the optimization algorithm.\n",
        "Note that this is the same as we applied in the linear regression example and it illustrates the general applicability of the optimizers.\n"
      ],
      "metadata": {
        "id": "V1HKsZ_yE_GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = tf.keras.optimizers.SGD(learning_rate=.1)"
      ],
      "metadata": {
        "id": "8v7EvZCP9wtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "Next we **call the training function** to train the model."
      ],
      "metadata": {
        "id": "8MjK56-eFDG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_model(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "ElBsC4tr9wwN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "1ef4b187-fa05-44ba-dc87-e5e900cbc592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 238.965625 180.65625\" width=\"238.965625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 238.965625 180.65625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 143.1 \nL 225.403125 143.1 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 51.803125 143.1 \nL 51.803125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mcf53a6bc0e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.803125\" xlink:href=\"#mcf53a6bc0e\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(48.621875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 95.203125 143.1 \nL 95.203125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"95.203125\" xlink:href=\"#mcf53a6bc0e\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(92.021875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 138.603125 143.1 \nL 138.603125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.603125\" xlink:href=\"#mcf53a6bc0e\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(135.421875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 182.003125 143.1 \nL 182.003125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.003125\" xlink:href=\"#mcf53a6bc0e\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(178.821875 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 225.403125 143.1 \nL 225.403125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.403125\" xlink:href=\"#mcf53a6bc0e\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(219.040625 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(112.525 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 30.103125 120.45 \nL 225.403125 120.45 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb7955dd31d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mb7955dd31d\" y=\"120.45\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 124.249219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 30.103125 75.15 \nL 225.403125 75.15 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mb7955dd31d\" y=\"75.15\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 78.949219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 30.103125 29.85 \nL 225.403125 29.85 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mb7955dd31d\" y=\"29.85\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 33.649219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 30.103125 33.032279 \nL 51.803125 81.999538 \nL 73.503125 92.068897 \nL 95.203125 97.68553 \nL 116.903125 101.309068 \nL 138.603125 103.828251 \nL 160.303125 105.74321 \nL 182.003125 107.559001 \nL 203.703125 108.705122 \nL 225.403125 109.724157 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 30.103125 41.07685 \nL 51.803125 26.803575 \nL 73.503125 23.987425 \nL 95.203125 22.545375 \nL 116.903125 21.37135 \nL 138.603125 20.642775 \nL 160.303125 19.902875 \nL 182.003125 19.5518 \nL 203.703125 19.238475 \nL 225.403125 18.89495 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pf6a0946eb0)\" d=\"M 30.103125 32.09235 \nL 51.803125 28.0833 \nL 73.503125 25.97685 \nL 95.203125 24.1422 \nL 116.903125 23.508 \nL 138.603125 24.16485 \nL 160.303125 23.07765 \nL 182.003125 22.35285 \nL 203.703125 22.12635 \nL 225.403125 22.48875 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 143.1 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 143.1 \nL 225.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 143.1 \nL 225.403125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 140.634375 98.667187 \nL 218.403125 98.667187 \nQ 220.403125 98.667187 220.403125 96.667187 \nL 220.403125 53.632812 \nQ 220.403125 51.632812 218.403125 51.632812 \nL 140.634375 51.632812 \nQ 138.634375 51.632812 138.634375 53.632812 \nL 138.634375 96.667187 \nQ 138.634375 98.667187 140.634375 98.667187 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 142.634375 59.73125 \nL 162.634375 59.73125 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_10\">\n     <!-- train loss -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(170.634375 63.23125)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 142.634375 74.409375 \nL 162.634375 74.409375 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_11\">\n     <!-- train acc -->\n     <g transform=\"translate(170.634375 77.909375)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 142.634375 89.0875 \nL 162.634375 89.0875 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_12\">\n     <!-- test acc -->\n     <g transform=\"translate(170.634375 92.5875)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"223.828125\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"285.107422\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"340.087891\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf6a0946eb0\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, this algorithm converges to a solution\n",
        "that achieves a decent accuracy,\n",
        "albeit this time with fewer lines of code than before.\n",
        "\n",
        "\n",
        "### Summary of the second approach\n",
        "\n",
        "* Using high-level APIs, we can implement softmax regression much more concisely.\n",
        "* From a computational perspective, implementing softmax regression has intricacies. Note that in many cases, a deep learning framework takes additional precautions beyond these most well-known tricks to ensure numerical stability, saving us from even more pitfalls that we would encounter if we tried to code all of our models from scratch in practice.\n"
      ],
      "metadata": {
        "id": "fYigQXUUFPnq"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}